{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d93db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re \n",
    "from collections import Counter\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83e6fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(email_lines):\n",
    "    subjects = []\n",
    "    contents = []\n",
    "\n",
    "    for text in email_lines:\n",
    "        text = text.strip()\n",
    "        match = re.match(r\"Subject:\\s*([^\\(\\)]+?)\\s*:\\s*(.+)\", text)\n",
    "        if match:\n",
    "            possible_tag = match.group(1).strip()\n",
    "            remaining = match.group(2).strip()\n",
    "            subjects.append(possible_tag)\n",
    "            contents.append(remaining)\n",
    "            continue\n",
    "        subjects.append(\"\")\n",
    "        contents.append(text)\n",
    "\n",
    "    return subjects, contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c28a1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('archive.zip') as zipObject:\n",
    "    with zipObject.open('emails.csv') as csvObject: \n",
    "        df = pd.read_csv(csvObject)\n",
    "X , y = df[\"text\"] , df[\"spam\"] \n",
    "\n",
    "X_subject , X_content = extract_subject(X) \n",
    "count = Counter(X_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73a30ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X):\n",
    "    X_lower = [data.lower() for data in X]\n",
    "\n",
    "    X_nonspecial = []\n",
    "    for data in X_lower:\n",
    "        cleaned = re.sub(r'[^a-zA-Z\\s]', '', data)\n",
    "        X_nonspecial.append(cleaned)\n",
    "\n",
    "    X_tokenize = [data.split() for data in X_nonspecial]\n",
    "\n",
    "    total_tokens = []\n",
    "    for tokens in X_tokenize:\n",
    "        total_tokens.extend(tokens)\n",
    "\n",
    "    count = Counter(total_tokens).most_common(2000)\n",
    "    return count , X_lower , X_nonspecial , X_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequency,X_lower,X_nonspecial,X_tokenize = data_preprocessing(X_content)\n",
    "dictionary  = {token_frequency[i][0]: i+1 for i in range(len(token_frequency))}\n",
    "X_lower \n",
    "def split_sentences(X,min_sentence_lenght = 8):\n",
    "    total_sentences = [] \n",
    "    for text in X: \n",
    "        sentences = re.split(r'[.?,]+\\s*',text)\n",
    "        for s in sentences: \n",
    "            cleaned_sentences = re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "            cleaned_sentences = re.sub(r'\\b[a-zA-Z]\\b', '', cleaned_sentences)\n",
    "            cleaned_sentences = re.sub(r'\\s+',' ',cleaned_sentences)\n",
    "            if(len(cleaned_sentences.strip().split(' ')) > min_sentence_lenght):\n",
    "                total_sentences.append(cleaned_sentences.strip())\n",
    "    return total_sentences\n",
    "sentences = split_sentences(X_lower)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313031bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(sentences):\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    \n",
    "    corrected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        corrected_words = []\n",
    "        words = sentence.split()  \n",
    "        for word in words:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                corrected_words.append(suggestions[0].term) \n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        corrected_sentences.append(' '.join(corrected_words))\n",
    "    return corrected_sentences\n",
    "\n",
    "sentences = [\"hi i studi in schooo for eleven hourse\"]\n",
    "corrected = spell_checker(sentences)\n",
    "print(corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('cv-unique-has-end-punct-sentences.csv.zip') as zip_object:\n",
    "    with zip_object.open('cv-unique-has-end-punct-sentences.csv') as file_object:\n",
    "        df = pd.read_csv(file_object)\n",
    "\n",
    "sentences = df['sentence']\n",
    "lower = [data.lower() for data in sentences]\n",
    "\n",
    "cleaned_sentences = split_sentences(lower)\n",
    "\n",
    "frequency_table, _, _, _ = data_preprocessing(cleaned_sentences)\n",
    "frequency_dict = {key: index for index, (key, value) in enumerate(frequency_table)}\n",
    "vocab_size = len(frequency_dict)\n",
    "\n",
    "def make_pairs_to_train(cleaned_sentences, frequency_dict, max_length):\n",
    "    pairs = []\n",
    "    for sentence in cleaned_sentences:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            w1, w2 = words[i], words[i + 1]\n",
    "            if w1 in frequency_dict and w2 in frequency_dict:\n",
    "                pairs.append((w1, w2))\n",
    "            if len(pairs) >= max_length:\n",
    "                return pairs\n",
    "    return pairs\n",
    "\n",
    "pairs = make_pairs_to_train(cleaned_sentences, frequency_dict, 30000)\n",
    "\n",
    "def one_hot(index, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "dataset = []\n",
    "for w1, w2 in pairs:\n",
    "    idx1 = frequency_dict[w1]\n",
    "    idx2 = frequency_dict[w2]\n",
    "    input_vector = one_hot(idx1, vocab_size)\n",
    "    target_vector = one_hot(idx2, vocab_size)\n",
    "    dataset.append((input_vector, target_vector))\n",
    "pairs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b152ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWTrainer:\n",
    "    def __init__(self, data, epochs, vocab_size, hidden_size, learning_rate):\n",
    "        self.data = data \n",
    "        self.epochs = epochs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_input = self.initialize_weights(vocab_size, hidden_size)\n",
    "        self.weights_output = self.initialize_weights(hidden_size, vocab_size)\n",
    "\n",
    "    def initialize_weights(self, input_size, output_size):\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        return np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x = x - np.max(x)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x)\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "\n",
    "    def forward(self, input_vector, target_vector):\n",
    "        hidden_activation = np.dot(input_vector, self.weights_input)\n",
    "        output_scores = np.dot(hidden_activation, self.weights_output)  \n",
    "        y_pred = self.softmax(output_scores)\n",
    "        loss = self.cross_entropy_loss(y_pred, target_vector)\n",
    "        return y_pred, loss, hidden_activation\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for input_vector, target_vector in self.data:\n",
    "                y_pred, loss, hidden_activation = self.forward(input_vector, target_vector)\n",
    "\n",
    "                error = y_pred - target_vector  \n",
    "                dW2 = np.outer(hidden_activation, error) \n",
    "                dW1 = np.outer(input_vector, np.dot(self.weights_output, error)) \n",
    "                self.weights_input -= self.learning_rate * dW1\n",
    "                self.weights_output -= self.learning_rate * dW2\n",
    "\n",
    "                total_loss += loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def convert_(self, data): \n",
    "        if np.sum(data) == 0:\n",
    "            print(\"This word does not exist in the vocab.\")\n",
    "        return np.dot(data, self.weights_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtovec = CBOWTrainer(dataset,20,2000,10,0.05)\n",
    "wordtovec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2160d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attempt_2 = one_hot(frequency_dict['good'],vocab_size)\n",
    "attempt_3 = one_hot(frequency_dict['bad'],vocab_size) \n",
    "alpha , beta  = wordtovec.convert_(attempt_2),wordtovec.convert_(attempt_3)\n",
    "def cosine_similarity(a,b): \n",
    "    a_dot_b = np.dot(a,b)\n",
    "    return np.abs((a_dot_b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "print(\"the cosine similarity is \" , cosine_similarity(alpha,beta)) \n",
    "alpha , beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7228e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ecbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_:\n",
    "    def __init__(self, x_data, cutoff=0.95, kernel=None, degree=3, n_greater=False):\n",
    "        self.data = x_data\n",
    "        self.data_size, self.dims = x_data.shape\n",
    "        self.cutoff = cutoff\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.n_greater = n_greater\n",
    "\n",
    "        self.top_eigen_pairs = None\n",
    "        self.mean = None\n",
    "\n",
    "        self.eigen_vector_pairs = None \n",
    "        self.cutoff_point = 0 \n",
    "\n",
    "    def _center_data(self):\n",
    "        self.mean = np.mean(self.data, axis=0)\n",
    "        return self.data - self.mean\n",
    "    \n",
    "    def _center_kernel(self,kernel): \n",
    "        n = kernel.shape[0]\n",
    "        one_n = np.ones((n,n))/n \n",
    "        k_centered = kernel - one_n @kernel - kernel@one_n + one_n@one_n\n",
    "        return k_centered\n",
    "     \n",
    "    def _kernel_transform(self, X):\n",
    "        if self.kernel is None and self.n_greater:\n",
    "            return np.dot(X , X.T) #already centerd due to centering of the input data\n",
    "        elif self.kernel is None and self.n_greater is False: \n",
    "            return np.dot(X.T , X)\n",
    "        elif self.kernel == \"poly\":\n",
    "            return self._center_kernel((np.dot(X, X.T) + 1) ** self.degree)\n",
    "        elif self.kernel == \"rbf\":\n",
    "            gamma = 1 / X.shape[1]\n",
    "            sq_dists = np.sum(X ** 2, axis=1).reshape(-1, 1) + \\\n",
    "                       np.sum(X ** 2, axis=1) - 2 * np.dot(X, X.T)\n",
    "            return self._center_kernel(np.exp(-gamma * sq_dists))\n",
    "        \n",
    "    def _cutoff(self, sorted_eigen_values):\n",
    "        total = np.sum(sorted_eigen_values)\n",
    "        accum = 0\n",
    "        for j in range(len(sorted_eigen_values)):\n",
    "            accum += sorted_eigen_values[j]\n",
    "            if accum / total >= self.cutoff:\n",
    "                return j + 1  # +1 to include this component\n",
    "        return len(sorted_eigen_values)  # fallback in case total not reached\n",
    "        \n",
    "    def train(self):\n",
    "        X = self._center_data() # centering dataset \n",
    "        centered_kernel  = self._kernel_transform(X) # returns the centered_kernel for the dataset (if kernel = \"None\" returns the covarience matrix)\n",
    "\n",
    "        # till now i am working with (which can lead to large time complexity)\n",
    "\n",
    "        if(self.kernel is None and self.n_greater is False):\n",
    "            # do straight up calculation of the covarience matrix and store the eigen directions and the cutoff point\n",
    "\n",
    "            eigen_vals, eigen_vecs = np.linalg.eigh(self.kernel)\n",
    "            sorted_idx = np.argsort(eigen_vals)[::-1]\n",
    "\n",
    "            eigen_vals = eigen_vals[sorted_idx]\n",
    "            eigen_vecs = eigen_vecs[:, sorted_idx]\n",
    "\n",
    "            self.cutoff_point = self._cutoff(eigen_vals)\n",
    "            \n",
    "            top_vals = eigen_vals[:self.cutoff_point]\n",
    "            top_vecs = eigen_vecs[:, :self.cutoff_point]\n",
    "\n",
    "            # Normalize eigenvectors (each column vector to unit norm)\n",
    "            norms = np.linalg.norm(top_vecs, axis=0, keepdims=True)\n",
    "            top_vecs_normalized = top_vecs / norms\n",
    "\n",
    "            self.components = top_vecs_normalized\n",
    "            pass\n",
    "        elif (self.kernel is None and self.n_greater is True): \n",
    "            # find the eigen values and directions to get the coefficient and then find the eigen directions with that \n",
    "            eigen_vals, eigen_vecs = np.linalg.eigh(self.kernel)\n",
    "            sorted_idx = np.argsort(eigen_vals)[::-1]\n",
    "\n",
    "            eigen_vals = eigen_vals[sorted_idx]\n",
    "            eigen_vecs = eigen_vecs[:, sorted_idx]\n",
    "\n",
    "            # Normalize eigenvectors of G by sqrt of eigenvalues\n",
    "            eigen_vecs_norm = eigen_vecs / np.sqrt(eigen_vals + 1e-10)\n",
    "\n",
    "            # Convert coefficients to eigen directions in original space\n",
    "            eigen_dirs = X.T @ eigen_vecs_norm  # d x n\n",
    "\n",
    "            # Now apply cutoff on eigenvalues and corresponding directions\n",
    "            self.cutoff_point = self._cutoff(eigen_vals)\n",
    "            top_vals = eigen_vals[:self.cutoff_point]\n",
    "            top_dirs = eigen_dirs[:, :self.cutoff_point]\n",
    "\n",
    "            # Normalize eigen directions to unit length (optional but recommended)\n",
    "            norms = np.linalg.norm(top_dirs, axis=0, keepdims=True)\n",
    "            self.components = top_dirs / norms\n",
    "\n",
    "            # Save info for transform\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            pass\n",
    "        else: \n",
    "            # store all the datapoints and then when a new datapoint comes in find the sum(coefficient*kernel(datapoint , xi))\n",
    "            eigen_vals, eigen_vecs = np.linalg.eigh(self.kernel)\n",
    "            sorted_idx = np.argsort(eigen_vals)[::-1]\n",
    "\n",
    "            eigen_vals = eigen_vals[sorted_idx]\n",
    "            eigen_vecs = eigen_vecs[:, sorted_idx]\n",
    "\n",
    "            self.cutoff_point = self._cutoff(eigen_vals)\n",
    "\n",
    "            top_vals = eigen_vals[:self.cutoff_point]\n",
    "            # Normalize top eigenvectors directly here\n",
    "            top_vecs = eigen_vecs[:, :self.cutoff_point] / np.sqrt(top_vals + 1e-10)\n",
    "\n",
    "            self.alphas = top_vecs\n",
    "            self.X_fit = X\n",
    "            pass\n",
    "\n",
    "    def transform(self, X_new):\n",
    "        if self.kernel is None and self.n_greater is False:\n",
    "            # Linear PCA\n",
    "            X_centered = X_new - np.mean(self.X_fit, axis=0)\n",
    "            return X_centered @ self.components\n",
    "\n",
    "        elif self.kernel is None and self.n_greater is True:\n",
    "            # Dual PCA\n",
    "            X_centered = X_new - self.mean\n",
    "            return X_centered @ self.components\n",
    "\n",
    "        else:\n",
    "            # Kernel PCA\n",
    "            # Compute kernel matrix between X_new and training samples\n",
    "            K_new = np.array([[self.kernel(x_new, x_train) for x_train in self.X_fit] for x_new in X_new])\n",
    "            return K_new @ self.alphas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df150a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
